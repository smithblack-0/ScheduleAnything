{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scheduling Logical Batch Size in PyTorch\n",
        "\n",
        "This example demonstrates extending existing optimizers using schedule anything, by implementing a flavor of logical batch size using scheduling.\n",
        "\n",
        "Pay attention to the decoupling of concerns this can bring about."
      ],
      "metadata": {
        "id": "btmKKFh5BPXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup and Imports\n",
        "\n",
        "We use magic commands to ensure the environment is setup. Then we run all the needed imports. Note the usage of the cannonical ScheduleAnything import pattern:  torch-schedule-anything -> tsa\n",
        "\n",
        "```\n",
        "import torch_schedule_anything as tsa\n",
        "```"
      ],
      "metadata": {
        "id": "jTSm9yFyBgl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install -q transformers datasets torch-schedule-anything torch\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_schedule_anything as tsa\n",
        "\n",
        "# Type hints\n",
        "from torch_schedule_anything import SynchronousSchedule\n",
        "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ],
      "metadata": {
        "id": "MxJY5n5-B_Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "For easy experimentation, we place the majority of the hyperparameters right here, though we do hardwire the dataset. For the most part, we stick to some fairly boring configurations that should be familiar boilerplate to anyone in NLP.\n",
        "\n",
        "Training duration and details are specified in terms of number of batches, learning rate has been set to something that is known to train, and the schedules are functional.\n",
        "\n",
        "### Schedule Overview\n",
        "\n",
        "Scheduling using builtins in this library generally works by specifying a number of warmup steps (in this case batches) a number of training steps, and some parameters relating to warmup targets and values.\n",
        "\n",
        "It should always be kept in mind that torch schedules are applied in terms of\n",
        "value(t) = base_hyperparameter*lambda(t), meaning you will get the base value times a multiplier as your final rate.\n",
        "\n",
        "The warmup target tells you what lambda will be when warmup finishes, while the final target tells you what it will be at end of training. Largely, the various builtin curves say how we get there. In this case, we use a cosine annealing, and a quadratic curve for learning rate and weight decay respectively.\n",
        "\n",
        "### Schedule Config\n",
        "\n",
        "We are going to schedule logical batch size. This is largely inspired by smith's work, but does not use his exact algorithm, as this is simply a demonstation.\n",
        "\n",
        "### Tuning and Purpose\n",
        "\n",
        "This exists primarily to demonstrate the technology, not demonstrate a well-tuned example. This example has not been properly tuned besides verifying convergence, and as such do not treat this as having been deployed to be optimal."
      ],
      "metadata": {
        "id": "5id3Fv4nB9-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Device\n",
        "LOGGING_RATE = 5 # How frequently to print to console\n",
        "\n",
        "# Model/Pipeline config\n",
        "MODEL_NAME = \"distilbert-base-uncased\" # Model\n",
        "MAX_LENGTH = 256 # Maximum number of tokens in sample\n",
        "BATCH_SIZE = 8 # Batches in samples\n",
        "TOTAL_BATCHES = 10000 # All batches used over training\n",
        "WARMUP_BATCHES = 500 # Number of batches used for warmup\n",
        "\n",
        "# The learning rate and schedule.\n",
        "#\n",
        "# We will warm up and just become a constant.\n",
        "BASE_LR = 0.00001\n",
        "BASE_WD = 0.01\n",
        "\n",
        "# The batch size schedule\n",
        "# is a quadratic schedule from low to high\n",
        "STARTING_BATCH_SIZE = 16\n",
        "ENDING_BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "WdDeMlizGY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Boilerplate\n",
        "\n",
        "Largely standard boilerplate here.\n",
        "We make a model, we make an AdamW optimizer,\n",
        "we make a pipeline that loads imdb and tokenizes it"
      ],
      "metadata": {
        "id": "XN4oRZ5xHP-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model()->PreTrainedModel:\n",
        "    \"\"\"Load pretrained model with classification head.\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=2\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def make_dataloader()->DataLoader:\n",
        "    \"\"\"Load and tokenize IMDB dataset, return DataLoader.\"\"\"\n",
        "    dataset = load_dataset(\"imdb\", split=\"train\")  # Subset for faster demo\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def tokenize(examples):\n",
        "        result = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        result[\"labels\"] = examples[\"label\"]\n",
        "        return result\n",
        "\n",
        "    dataset = dataset.map(tokenize, batched=True)\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "def make_optimizer(model: PreTrainedModel)->Optimizer:\n",
        "    \"\"\"Create optimizer with base hyperparameter values that schedules multiply against.\"\"\"\n",
        "    return AdamW(\n",
        "        model.parameters(),\n",
        "        lr=BASE_LR,\n",
        "        weight_decay=BASE_WD\n",
        "    )"
      ],
      "metadata": {
        "id": "PIZr4sAAHaxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schedule Factory, and the Novelty\n",
        "\n",
        "This is where ScheduleAnything comes in. We're going to bind a new field to the optimizer, bind two schedules, and define a helper that can tell when it is time to step.\n",
        "\n",
        "**The pattern:**\n",
        "1. Add optimizer fields as needed\n",
        "2. Create a schedule for each hyperparameter you want to control\n",
        "3. Use `schedule_target` to specify which hyperparameter each schedule controls\n",
        "4. Wrap them in `SynchronousSchedule` to keep them coordinated\n",
        "5. Define utilities in the same place using tsa that respond to your extra hyperparameters to invoke in the training loop.\n",
        "\n",
        "**Crucially**, this means downstream can access through well abstracted utilities, maintaining separation of concern."
      ],
      "metadata": {
        "id": "IDBw6SCFHnLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_schedule(optimizer: Optimizer)->SynchronousSchedule:\n",
        "    \"\"\"\n",
        "    Create coordinated schedules for learning rate and weight decay.\n",
        "\n",
        "    Returns a SynchronousSchedule that steps both schedules together.\n",
        "    \"\"\"\n",
        "    # Learning rate: cosine annealing\n",
        "    lr_scheduler = tsa.constant_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=1.0, # Base learning rate already encoded\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        schedule_target='lr'\n",
        "    )\n",
        "\n",
        "    # Create the logical batch feature in the first place,\n",
        "    # then bind the schedule\n",
        "\n",
        "    tsa.extend_optimizer(optimizer,\n",
        "                         name=\"logical_batch_size_target\",\n",
        "                         default_value=1.0)\n",
        "\n",
        "    batch_size_scheduler = tsa.quadratic_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=STARTING_BATCH_SIZE,\n",
        "        anneal_to_value=ENDING_BATCH_SIZE,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        schedule_target='logical_batch_size_target'\n",
        "    )\n",
        "\n",
        "    # Coordinate them to step together\n",
        "    return tsa.SynchronousSchedule([lr_scheduler, batch_size_scheduler])\n",
        "\n"
      ],
      "metadata": {
        "id": "aBZiNufYH9fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accum_threshold(optimizer: Optimizer)->float:\n",
        "    \"\"\"\n",
        "    Gets the relevant threshold out of the optimizer using\n",
        "    utils\n",
        "    \"\"\"\n",
        "    thresholds = []\n",
        "    for value, _, _ in tsa.get_param_groups_regrouped_by_key(optimizer, 'logical_batch_size_target'):\n",
        "        thresholds.append(value)\n",
        "    return max(thresholds)/BATCH_SIZE"
      ],
      "metadata": {
        "id": "pxi5fEOFGxNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop\n",
        "Standard PyTorch training loop as used in NLP, with schedules per batch. We abstract away the changes to logging, however."
      ],
      "metadata": {
        "id": "4GnuGjxiLZcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_progress(schedule: SynchronousSchedule,\n",
        "                    batch_idx: int,\n",
        "                    loss: float,\n",
        "                    last_step_size: int):\n",
        "    last_lr = schedule.get_last_lr()[0]\n",
        "    last_batch_target = schedule.get_last_schedule('logical_batch_size_target')[0]\n",
        "    msg = (f\"Batch {batch_idx+1:4d}/{TOTAL_BATCHES}\"\n",
        "          f\" | Loss: {loss.item():.4f}\"\n",
        "          f\" | LR: {last_lr:.4e}\"\n",
        "          f\" | Target Batch Size: {last_batch_target:.4f}\"\n",
        "          f\" | Last Batch Size: {last_step_size}\"\n",
        "          )\n",
        "    print(msg)\n"
      ],
      "metadata": {
        "id": "hy0WVNLw3XAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: PreTrainedModel,\n",
        "          dataloader: DataLoader,\n",
        "          optimizer: Optimizer,\n",
        "          schedule: LRScheduler,\n",
        "          ):\n",
        "    \"\"\"Train for TOTAL_BATCHES batches.\"\"\"\n",
        "    model.train()\n",
        "    data_iter = iter(dataloader)\n",
        "    accum_threshold = 1\n",
        "    accum_steps = 0\n",
        "    last_batch_size = 0\n",
        "\n",
        "    for batch_idx in range(TOTAL_BATCHES):\n",
        "        # Get next batch\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dataloader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        # Forward pass and backwards pass\n",
        "        # Increase batch size\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss = loss/max(int(accum_threshold), 1)\n",
        "        loss.backward()\n",
        "        accum_steps += 1\n",
        "\n",
        "        # Optimizer steps when I hit or exceed my target\n",
        "        if accum_steps >= accum_threshold:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            accum_threshold = get_accum_threshold(optimizer)\n",
        "            last_batch_size = accum_steps * BATCH_SIZE\n",
        "            accum_steps = 0\n",
        "\n",
        "        # Step schedules\n",
        "        schedule.step()\n",
        "\n",
        "        # Log progress\n",
        "        if (batch_idx + 1) % LOGGING_RATE == 0:\n",
        "            assert len(schedule.get_last_lr()) == 1, \"update logging system when adding param groups\"\n",
        "            report_progress(schedule, batch_idx, loss, last_batch_size)\n"
      ],
      "metadata": {
        "id": "WuKa4oRqLvbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting It All Together\n",
        "\n",
        "Create the components and train.\n"
      ],
      "metadata": {
        "id": "qifljNXpLwa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Setting up model and data...\")\n",
        "    model = make_model()\n",
        "    dataloader = make_dataloader()\n",
        "\n",
        "    print(\"Creating optimizer and schedules...\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    schedule = make_schedule(optimizer)\n",
        "\n",
        "    #print(f\"Scheduling: {schedule.schedule_names}\")\n",
        "    print(f\"Training for {TOTAL_BATCHES} batches with {WARMUP_BATCHES} warmup\")\n",
        "    print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "    train(model, dataloader, optimizer, schedule)\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "K7E4djwcLt6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}