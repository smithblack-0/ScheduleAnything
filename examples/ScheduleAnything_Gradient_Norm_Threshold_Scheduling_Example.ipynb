{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scheduling GNTS in PyTorch\n",
        "\n",
        "This example demonstrates the implementation of the Gradient Norm Threshold Scheduling algorithm orthogonal to an existing Adam optimizer. The implementation responds to a scheduled value and asks for the gradient norms to be below a threshold before taking a step."
      ],
      "metadata": {
        "id": "btmKKFh5BPXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup and Imports\n",
        "\n",
        "We use magic commands to ensure the environment is setup. Then we run all the needed imports. Note the usage of the cannonical ScheduleAnything import pattern:  torch-schedule-anything -> tsa\n",
        "\n",
        "```\n",
        "import torch_schedule_anything as tsa\n",
        "```"
      ],
      "metadata": {
        "id": "jTSm9yFyBgl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install -q transformers datasets torch-schedule-anything torch\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_schedule_anything as tsa\n",
        "\n",
        "# Type hints\n",
        "from torch_schedule_anything import SynchronousSchedule\n",
        "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ],
      "metadata": {
        "id": "MxJY5n5-B_Nw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "For easy experimentation, we place the majority of the hyperparameters right here, though we do hardwire the dataset. For the most part, we stick to some fairly boring configurations that should be familiar boilerplate to anyone in NLP.\n",
        "\n",
        "Training duration and details are specified in terms of number of batches, learning rate has been set to something that is known to train, and the schedules are functional.\n",
        "\n",
        "### Schedule Overview\n",
        "\n",
        "Scheduling using builtins in this library generally works by specifying a number of warmup steps (in this case batches) a number of training steps, and some parameters relating to warmup targets and values.\n",
        "\n",
        "It should always be kept in mind that torch schedules are applied in terms of\n",
        "value(t) = base_hyperparameter*lambda(t), meaning you will get the base value times a multiplier as your final rate.\n",
        "\n",
        "The warmup target tells you what lambda will be when warmup finishes, while the final target tells you what it will be at end of training. Largely, the various builtin curves say how we get there. In this case, we use a cosine annealing, and a quadratic curve for learning rate and weight decay respectively.\n",
        "\n",
        "### Schedule Config\n",
        "\n",
        "We are going to schedule logical batch size. This is largely inspired by smith's work, but does not use his exact algorithm, as this is simply a demonstation.\n",
        "\n",
        "### Tuning and Purpose\n",
        "\n",
        "This exists primarily to demonstrate the technology, not demonstrate a well-tuned example. This example has not been properly tuned besides verifying convergence, and as such do not treat this as having been deployed to be optimal."
      ],
      "metadata": {
        "id": "5id3Fv4nB9-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Device\n",
        "LOGGING_RATE = 50 # How frequently to print to console\n",
        "\n",
        "# Model/Pipeline config\n",
        "MODEL_NAME = \"distilbert-base-uncased\" # Model\n",
        "MAX_LENGTH = 256 # Maximum number of tokens in sample\n",
        "BATCH_SIZE = 8 # Batches in samples\n",
        "TOTAL_BATCHES = 30000 # All batches used over training\n",
        "WARMUP_BATCHES = 4000 # Number of batches used for warmup\n",
        "\n",
        "# The learning rate/weight decay/norm base details\n",
        "BASE_LR = 6e-5\n",
        "BASE_WD = 0.01\n",
        "BASE_NORM = 1.0\n",
        "\n",
        "ANNEALING_START_SCHEDULE = 1.0\n",
        "ANNEALING_END_SCHEDULE = 0.01\n",
        "\n",
        "# The threshold annealing instead proceeds as...\n",
        "\n",
        "WARMUP_MULTIPLIER = 20.0\n",
        "THRESHOLD_START_SCHEDULE = 0.95\n",
        "THRESHOLD_END_SCHEDULE = 0.25"
      ],
      "metadata": {
        "id": "WdDeMlizGY_w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Boilerplate\n",
        "\n",
        "Largely standard boilerplate here.\n",
        "We make a model, we make an AdamW optimizer,\n",
        "we make a pipeline that loads imdb and tokenizes it"
      ],
      "metadata": {
        "id": "XN4oRZ5xHP-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model()->PreTrainedModel:\n",
        "    \"\"\"Load pretrained model with classification head.\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=2\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def make_dataloader()->DataLoader:\n",
        "    \"\"\"Load and tokenize IMDB dataset, return DataLoader.\"\"\"\n",
        "    dataset = load_dataset(\"imdb\", split=\"train\")  # Subset for faster demo\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def tokenize(examples):\n",
        "        result = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        result[\"labels\"] = examples[\"label\"]\n",
        "        return result\n",
        "\n",
        "    dataset = dataset.map(tokenize, batched=True)\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "def make_optimizer(model: PreTrainedModel)->Optimizer:\n",
        "    \"\"\"Create optimizer with base hyperparameter values that schedules multiply against.\"\"\"\n",
        "    return AdamW(\n",
        "        model.parameters(),\n",
        "        lr=BASE_LR,\n",
        "        weight_decay=BASE_WD\n",
        "    )"
      ],
      "metadata": {
        "id": "PIZr4sAAHaxj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schedule Factory, and the Novelty\n",
        "\n",
        "This is where ScheduleAnything comes in. We're going to bind a new field to the optimizer, bind two schedules, and define a helper that can tell when it is time to step.\n",
        "\n",
        "**The pattern:**\n",
        "1. Add optimizer fields as needed\n",
        "2. Create a schedule for each hyperparameter you want to control\n",
        "3. Use `schedule_target` to specify which hyperparameter each schedule controls\n",
        "4. Wrap them in `SynchronousSchedule` to keep them coordinated\n",
        "5. Define utilities in the same place using tsa that respond to your extra hyperparameters to invoke in the training loop.\n",
        "\n",
        "**Crucially**, this means downstream can access through well abstracted utilities, maintaining separation of concern."
      ],
      "metadata": {
        "id": "IDBw6SCFHnLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_schedule(optimizer: Optimizer)->SynchronousSchedule:\n",
        "    \"\"\"\n",
        "    Create coordinated schedules for learning rate and weight decay.\n",
        "\n",
        "    Returns a SynchronousSchedule that steps both schedules together.\n",
        "    \"\"\"\n",
        "    # Extend optimizer to include threshold\n",
        "    tsa.extend_optimizer(optimizer,\n",
        "                         name=\"gradient_norm_threshold\",\n",
        "                         default_value=BASE_NORM)\n",
        "\n",
        "    # Learning rate: constant with warmup\n",
        "    lr_scheduler = tsa.constant_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=1.0, # Base learning rate already encoded\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        schedule_target='lr'\n",
        "    )\n",
        "\n",
        "    # Weight decay: Simulates normal learning rate annealing\n",
        "    wd_schedule = tsa.cosine_annealing_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=ANNEALING_START_SCHEDULE,\n",
        "        anneal_to_value=ANNEALING_END_SCHEDULE,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        schedule_target='weight_decay'\n",
        "    )\n",
        "\n",
        "    # Gradient threshold: Same schedule as weight deay\n",
        "    grad_schedule = tsa.cosine_annealing_with_inverse_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=THRESHOLD_START_SCHEDULE,\n",
        "        anneal_to_value=THRESHOLD_END_SCHEDULE,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        warmup_multiplier=WARMUP_MULTIPLIER,\n",
        "        schedule_target=\"gradient_norm_threshold\"\n",
        "    )\n",
        "\n",
        "    # Coordinate them to step together\n",
        "    return tsa.SynchronousSchedule([lr_scheduler, wd_schedule, grad_schedule])"
      ],
      "metadata": {
        "id": "aBZiNufYH9fy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad_norm_threshold(optimizer: Optimizer)->float:\n",
        "    \"\"\"Get the grad norm threshold used to decide step time\"\"\"\n",
        "    items = []\n",
        "    for value, _, _ in tsa.get_param_groups_regrouped_by_key(optimizer, 'gradient_norm_threshold'):\n",
        "        items.append(value)\n",
        "    return max(items)"
      ],
      "metadata": {
        "id": "Xd-Ixexxs9_G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad_norm(model: PreTrainedModel)->float:\n",
        "    \"\"\"\n",
        "    Gets the relevant norm out of the model using\n",
        "    torch utilities\n",
        "    \"\"\"\n",
        "    grads = [param.grad for param in model.parameters() if param.grad is not None]\n",
        "    return torch.nn.utils.get_total_norm(grads)"
      ],
      "metadata": {
        "id": "pZIofPLqtciv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop\n",
        "Standard PyTorch training loop as used in NLP, with schedules per batch. We abstract away the changes to logging, however."
      ],
      "metadata": {
        "id": "4GnuGjxiLZcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_progress(schedule: SynchronousSchedule,\n",
        "                    batch_idx: int,\n",
        "                    loss: float,\n",
        "                    norm: float,\n",
        "                    accum_steps: int\n",
        "                    ):\n",
        "    last_lr = schedule.get_last_lr()[0]\n",
        "    last_threshold = schedule.get_last_schedule(\"gradient_norm_threshold\")[0]\n",
        "    last_batch_size = BATCH_SIZE * accum_steps\n",
        "    msg = (f\"Batch {batch_idx+1:4d}/{TOTAL_BATCHES}\"\n",
        "          f\" | Loss: {loss.item():.4f}\"\n",
        "          f\" | LR: {last_lr:.4e}\"\n",
        "          f\" | Target_Threshold: {last_threshold:.4f}\"\n",
        "          f\" | Last Norm: {norm:.4f}\"\n",
        "          f\" | Last Accum Steps: {accum_steps}\"\n",
        "          f\" | Last Batch Size: {last_batch_size}\"\n",
        "          )\n",
        "    print(msg)\n"
      ],
      "metadata": {
        "id": "hy0WVNLw3XAu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: PreTrainedModel,\n",
        "          dataloader: DataLoader,\n",
        "          optimizer: Optimizer,\n",
        "          schedule: LRScheduler,\n",
        "          ):\n",
        "    \"\"\"Train for TOTAL_BATCHES batches.\"\"\"\n",
        "    model.train()\n",
        "    data_iter = iter(dataloader)\n",
        "    accum_steps = 0\n",
        "    last_norm = 0\n",
        "    last_num_accum_steps = 0\n",
        "\n",
        "    for batch_idx in range(TOTAL_BATCHES):\n",
        "        # Get next batch\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dataloader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        # Forward pass and backwards pass\n",
        "        # Increase batch size\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        accum_steps += 1\n",
        "\n",
        "        # Optimizer steps when I hit or exceed my target\n",
        "        # Fortunately, even for vector averages, f(a*x) = a*f(x), so we can rescale\n",
        "        # externally.\n",
        "        current_mean_norm = get_grad_norm(model)/accum_steps\n",
        "        if get_grad_norm_threshold(optimizer) >= current_mean_norm:\n",
        "            # Rescale gradients as mean by dividing\n",
        "            # by number of accum steps.\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad /= accum_steps\n",
        "\n",
        "            # Run update\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            last_num_accum_steps = accum_steps\n",
        "            last_norm = current_mean_norm\n",
        "            accum_steps = 0\n",
        "\n",
        "        # Step schedules\n",
        "        schedule.step()\n",
        "\n",
        "        # Log progress\n",
        "        if (batch_idx + 1) % LOGGING_RATE == 0:\n",
        "            assert len(schedule.get_last_lr()) == 1, \"update logging system when adding param groups\"\n",
        "            report_progress(schedule, batch_idx, loss, last_norm, last_num_accum_steps)\n"
      ],
      "metadata": {
        "id": "WuKa4oRqLvbB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting It All Together\n",
        "\n",
        "Create the components and train.\n"
      ],
      "metadata": {
        "id": "qifljNXpLwa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Setting up model and data...\")\n",
        "    model = make_model()\n",
        "    dataloader = make_dataloader()\n",
        "\n",
        "    print(\"Creating optimizer and schedules...\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    schedule = make_schedule(optimizer)\n",
        "\n",
        "    #print(f\"Scheduling: {schedule.schedule_names}\")\n",
        "    print(f\"Training for {TOTAL_BATCHES} batches with {WARMUP_BATCHES} warmup\")\n",
        "    print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "    train(model, dataloader, optimizer, schedule)\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7E4djwcLt6_",
        "outputId": "c694e9de-8493-4923-d6df-f259540a2cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up model and data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating optimizer and schedules...\n",
            "Training for 30000 batches with 4000 warmup\n",
            "Device: cuda\n",
            "\n",
            "Batch   50/30000 | Loss: 0.7246 | LR: 7.5000e-07 | Target_Threshold: 18.7744 | Last Norm: 2.8369 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  100/30000 | Loss: 0.6707 | LR: 1.5000e-06 | Target_Threshold: 18.5487 | Last Norm: 1.4487 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  150/30000 | Loss: 0.6903 | LR: 2.2500e-06 | Target_Threshold: 18.3231 | Last Norm: 1.2166 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  200/30000 | Loss: 0.6459 | LR: 3.0000e-06 | Target_Threshold: 18.0975 | Last Norm: 1.6951 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  250/30000 | Loss: 0.4775 | LR: 3.7500e-06 | Target_Threshold: 17.8719 | Last Norm: 8.4985 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  300/30000 | Loss: 0.3717 | LR: 4.5000e-06 | Target_Threshold: 17.6462 | Last Norm: 11.2185 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  350/30000 | Loss: 0.2966 | LR: 5.2500e-06 | Target_Threshold: 17.4206 | Last Norm: 10.0525 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  400/30000 | Loss: 0.2667 | LR: 6.0000e-06 | Target_Threshold: 17.1950 | Last Norm: 8.9185 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  450/30000 | Loss: 0.1094 | LR: 6.7500e-06 | Target_Threshold: 16.9694 | Last Norm: 2.3956 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  500/30000 | Loss: 0.1787 | LR: 7.5000e-06 | Target_Threshold: 16.7437 | Last Norm: 4.7733 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  550/30000 | Loss: 0.2525 | LR: 8.2500e-06 | Target_Threshold: 16.5181 | Last Norm: 12.3333 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  600/30000 | Loss: 0.6834 | LR: 9.0000e-06 | Target_Threshold: 16.2925 | Last Norm: 13.4520 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  650/30000 | Loss: 0.2852 | LR: 9.7500e-06 | Target_Threshold: 16.0669 | Last Norm: 5.9910 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  700/30000 | Loss: 0.1978 | LR: 1.0500e-05 | Target_Threshold: 15.8413 | Last Norm: 10.1691 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  750/30000 | Loss: 0.2271 | LR: 1.1250e-05 | Target_Threshold: 15.6156 | Last Norm: 8.6326 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  800/30000 | Loss: 0.5265 | LR: 1.2000e-05 | Target_Threshold: 15.3900 | Last Norm: 8.6357 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  850/30000 | Loss: 0.7439 | LR: 1.2750e-05 | Target_Threshold: 15.1644 | Last Norm: 8.6588 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch  900/30000 | Loss: 0.3379 | LR: 1.3500e-05 | Target_Threshold: 14.9387 | Last Norm: 7.8385 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch  950/30000 | Loss: 0.4528 | LR: 1.4250e-05 | Target_Threshold: 14.7131 | Last Norm: 8.7409 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1000/30000 | Loss: 0.6028 | LR: 1.5000e-05 | Target_Threshold: 14.4875 | Last Norm: 4.1122 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1050/30000 | Loss: 0.5249 | LR: 1.5750e-05 | Target_Threshold: 14.2619 | Last Norm: 7.8457 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1100/30000 | Loss: 0.0963 | LR: 1.6500e-05 | Target_Threshold: 14.0362 | Last Norm: 12.9237 | Last Accum Steps: 3 | Last Batch Size: 24\n",
            "Batch 1150/30000 | Loss: 0.3822 | LR: 1.7250e-05 | Target_Threshold: 13.8106 | Last Norm: 11.6709 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1200/30000 | Loss: 0.6764 | LR: 1.8000e-05 | Target_Threshold: 13.5850 | Last Norm: 11.3649 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1250/30000 | Loss: 0.1541 | LR: 1.8750e-05 | Target_Threshold: 13.3594 | Last Norm: 5.9255 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1300/30000 | Loss: 0.2650 | LR: 1.9500e-05 | Target_Threshold: 13.1337 | Last Norm: 7.4035 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1350/30000 | Loss: 0.5369 | LR: 2.0250e-05 | Target_Threshold: 12.9081 | Last Norm: 6.5378 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1400/30000 | Loss: 0.2583 | LR: 2.1000e-05 | Target_Threshold: 12.6825 | Last Norm: 10.7696 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1450/30000 | Loss: 0.1020 | LR: 2.1750e-05 | Target_Threshold: 12.4569 | Last Norm: 2.9223 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1500/30000 | Loss: 0.3248 | LR: 2.2500e-05 | Target_Threshold: 12.2312 | Last Norm: 10.4639 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1550/30000 | Loss: 0.2364 | LR: 2.3250e-05 | Target_Threshold: 12.0056 | Last Norm: 7.8350 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1600/30000 | Loss: 0.0639 | LR: 2.4000e-05 | Target_Threshold: 11.7800 | Last Norm: 10.4988 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 1650/30000 | Loss: 0.6019 | LR: 2.4750e-05 | Target_Threshold: 11.5544 | Last Norm: 9.0596 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1700/30000 | Loss: 0.4294 | LR: 2.5500e-05 | Target_Threshold: 11.3287 | Last Norm: 9.5037 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1750/30000 | Loss: 0.2528 | LR: 2.6250e-05 | Target_Threshold: 11.1031 | Last Norm: 7.1600 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1800/30000 | Loss: 0.4100 | LR: 2.7000e-05 | Target_Threshold: 10.8775 | Last Norm: 7.4781 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 1850/30000 | Loss: 0.4054 | LR: 2.7750e-05 | Target_Threshold: 10.6519 | Last Norm: 6.4689 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1900/30000 | Loss: 0.4505 | LR: 2.8500e-05 | Target_Threshold: 10.4262 | Last Norm: 9.1227 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 1950/30000 | Loss: 0.0718 | LR: 2.9250e-05 | Target_Threshold: 10.2006 | Last Norm: 3.3363 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 2000/30000 | Loss: 1.0893 | LR: 3.0000e-05 | Target_Threshold: 9.9750 | Last Norm: 8.3242 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 2050/30000 | Loss: 0.1958 | LR: 3.0750e-05 | Target_Threshold: 9.7494 | Last Norm: 8.6329 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 2100/30000 | Loss: 0.1019 | LR: 3.1500e-05 | Target_Threshold: 9.5237 | Last Norm: 8.5585 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 2150/30000 | Loss: 0.4912 | LR: 3.2250e-05 | Target_Threshold: 9.2981 | Last Norm: 9.2062 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 2200/30000 | Loss: 0.4245 | LR: 3.3000e-05 | Target_Threshold: 9.0725 | Last Norm: 7.1188 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 2250/30000 | Loss: 0.2805 | LR: 3.3750e-05 | Target_Threshold: 8.8469 | Last Norm: 6.2059 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 2300/30000 | Loss: 0.4123 | LR: 3.4500e-05 | Target_Threshold: 8.6212 | Last Norm: 2.6895 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 2350/30000 | Loss: 0.5358 | LR: 3.5250e-05 | Target_Threshold: 8.3956 | Last Norm: 7.5211 | Last Accum Steps: 3 | Last Batch Size: 24\n",
            "Batch 2400/30000 | Loss: 0.0786 | LR: 3.6000e-05 | Target_Threshold: 8.1700 | Last Norm: 4.5493 | Last Accum Steps: 1 | Last Batch Size: 8\n",
            "Batch 2450/30000 | Loss: 0.0297 | LR: 3.6750e-05 | Target_Threshold: 7.9444 | Last Norm: 5.5288 | Last Accum Steps: 3 | Last Batch Size: 24\n",
            "Batch 2500/30000 | Loss: 0.2411 | LR: 3.7500e-05 | Target_Threshold: 7.7188 | Last Norm: 7.4161 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 2550/30000 | Loss: 0.3240 | LR: 3.8250e-05 | Target_Threshold: 7.4931 | Last Norm: 6.5359 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 2600/30000 | Loss: 0.1812 | LR: 3.9000e-05 | Target_Threshold: 7.2675 | Last Norm: 5.9838 | Last Accum Steps: 2 | Last Batch Size: 16\n",
            "Batch 2650/30000 | Loss: 0.6462 | LR: 3.9750e-05 | Target_Threshold: 7.0419 | Last Norm: 6.2371 | Last Accum Steps: 3 | Last Batch Size: 24\n"
          ]
        }
      ]
    }
  ]
}