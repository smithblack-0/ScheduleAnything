{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scheduling GNTS in PyTorch\n",
        "\n",
        "This example demonstrates the implementation of the Gradient Norm Threshold Scheduling algorithm orthogonal to an existing Adam optimizer. The implementation responds to a scheduled value and asks for the gradient norms to be below a threshold before taking a step."
      ],
      "metadata": {
        "id": "btmKKFh5BPXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup and Imports\n",
        "\n",
        "We use magic commands to ensure the environment is setup. Then we run all the needed imports. Note the usage of the cannonical ScheduleAnything import pattern:  torch-schedule-anything -> tsa\n",
        "\n",
        "```\n",
        "import torch_schedule_anything as tsa\n",
        "```"
      ],
      "metadata": {
        "id": "jTSm9yFyBgl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install -q transformers datasets torch-schedule-anything torch\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_schedule_anything as tsa\n",
        "\n",
        "# Type hints\n",
        "from torch_schedule_anything import SynchronousSchedule\n",
        "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ],
      "metadata": {
        "id": "MxJY5n5-B_Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "For easy experimentation, we place the majority of the hyperparameters right here, though we do hardwire the dataset. For the most part, we stick to some fairly boring configurations that should be familiar boilerplate to anyone in NLP.\n",
        "\n",
        "Training duration and details are specified in terms of number of batches, learning rate has been set to something that is known to train, and the schedules are functional.\n",
        "\n",
        "### Schedule Overview\n",
        "\n",
        "Scheduling using builtins in this library generally works by specifying a number of warmup steps (in this case batches) a number of training steps, and some parameters relating to warmup targets and values.\n",
        "\n",
        "It should always be kept in mind that torch schedules are applied in terms of\n",
        "value(t) = base_hyperparameter*lambda(t), meaning you will get the base value times a multiplier as your final rate.\n",
        "\n",
        "The warmup target tells you what lambda will be when warmup finishes, while the final target tells you what it will be at end of training. Largely, the various builtin curves say how we get there. In this case, we use a cosine annealing, and a quadratic curve for learning rate and weight decay respectively.\n",
        "\n",
        "### Schedule Config\n",
        "\n",
        "We are going to schedule logical batch size. This is largely inspired by smith's work, but does not use his exact algorithm, as this is simply a demonstation.\n",
        "\n",
        "### Tuning and Purpose\n",
        "\n",
        "This exists primarily to demonstrate the technology, not demonstrate a well-tuned example. This example has not been properly tuned besides verifying convergence, and as such do not treat this as having been deployed to be optimal."
      ],
      "metadata": {
        "id": "5id3Fv4nB9-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Device\n",
        "LOGGING_RATE = 50 # How frequently to print to console\n",
        "\n",
        "# Model/Pipeline config\n",
        "MODEL_NAME = \"distilbert-base-uncased\" # Model\n",
        "MAX_LENGTH = 256 # Maximum number of tokens in sample\n",
        "BATCH_SIZE = 8 # Batches in samples\n",
        "TOTAL_BATCHES = 30000 # All batches used over training\n",
        "WARMUP_BATCHES = 4000 # Number of batches used for warmup\n",
        "\n",
        "# The learning rate/weight decay/norm base details\n",
        "BASE_LR = 6e-5\n",
        "BASE_WD = 0.01\n",
        "BASE_NORM = 1.0\n",
        "\n",
        "ANNEALING_START_SCHEDULE = 1.0\n",
        "ANNEALING_END_SCHEDULE = 0.01\n",
        "\n",
        "# The threshold annealing instead proceeds as...\n",
        "\n",
        "WARMUP_MULTIPLIER = 20.0\n",
        "THRESHOLD_START_SCHEDULE = 0.95\n",
        "THRESHOLD_END_SCHEDULE = 0.25"
      ],
      "metadata": {
        "id": "WdDeMlizGY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Boilerplate\n",
        "\n",
        "Largely standard boilerplate here.\n",
        "We make a model, we make an AdamW optimizer,\n",
        "we make a pipeline that loads imdb and tokenizes it"
      ],
      "metadata": {
        "id": "XN4oRZ5xHP-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model()->PreTrainedModel:\n",
        "    \"\"\"Load pretrained model with classification head.\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=2\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def make_dataloader()->DataLoader:\n",
        "    \"\"\"Load and tokenize IMDB dataset, return DataLoader.\"\"\"\n",
        "    dataset = load_dataset(\"imdb\", split=\"train\")  # Subset for faster demo\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def tokenize(examples):\n",
        "        result = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        result[\"labels\"] = examples[\"label\"]\n",
        "        return result\n",
        "\n",
        "    dataset = dataset.map(tokenize, batched=True)\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "def make_optimizer(model: PreTrainedModel)->Optimizer:\n",
        "    \"\"\"Create optimizer with base hyperparameter values that schedules multiply against.\"\"\"\n",
        "    return AdamW(\n",
        "        model.parameters(),\n",
        "        lr=BASE_LR,\n",
        "        weight_decay=BASE_WD\n",
        "    )"
      ],
      "metadata": {
        "id": "PIZr4sAAHaxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schedule Factory, and the Novelty\n",
        "\n",
        "This is where ScheduleAnything comes in. We're going to bind a new field to the optimizer, bind two schedules, and define a helper that can tell when it is time to step.\n",
        "\n",
        "**The pattern:**\n",
        "1. Add optimizer fields as needed\n",
        "2. Create a schedule for each hyperparameter you want to control\n",
        "3. Use `schedule_target` to specify which hyperparameter each schedule controls\n",
        "4. Wrap them in `SynchronousSchedule` to keep them coordinated\n",
        "5. Define utilities in the same place using tsa that respond to your extra hyperparameters to invoke in the training loop.\n",
        "\n",
        "**Crucially**, this means downstream can access through well abstracted utilities, maintaining separation of concern."
      ],
      "metadata": {
        "id": "IDBw6SCFHnLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_schedule(optimizer: Optimizer)->SynchronousSchedule:\n",
        "    \"\"\"\n",
        "    Create coordinated schedules for learning rate and weight decay.\n",
        "\n",
        "    Returns a SynchronousSchedule that steps both schedules together.\n",
        "    \"\"\"\n",
        "    # Extend optimizer to include threshold\n",
        "    tsa.extend_optimizer(optimizer,\n",
        "                         name=\"gradient_norm_threshold\",\n",
        "                         default_value=BASE_NORM)\n",
        "\n",
        "    # Learning rate: constant with warmup\n",
        "    lr_scheduler = tsa.constant_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=1.0, # Base learning rate already encoded\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        schedule_target='lr'\n",
        "    )\n",
        "\n",
        "    # Weight decay: Simulates normal learning rate annealing\n",
        "    wd_schedule = tsa.cosine_annealing_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=ANNEALING_START_SCHEDULE,\n",
        "        anneal_to_value=ANNEALING_END_SCHEDULE,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        schedule_target='weight_decay'\n",
        "    )\n",
        "\n",
        "    # Gradient threshold: Same schedule as weight deay\n",
        "    grad_schedule = tsa.cosine_annealing_with_inverse_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=THRESHOLD_START_SCHEDULE,\n",
        "        anneal_to_value=THRESHOLD_END_SCHEDULE,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        warmup_multiplier=WARMUP_MULTIPLIER,\n",
        "        schedule_target=\"gradient_norm_threshold\"\n",
        "    )\n",
        "\n",
        "    # Coordinate them to step together\n",
        "    return tsa.SynchronousSchedule([lr_scheduler, wd_schedule, grad_schedule])"
      ],
      "metadata": {
        "id": "aBZiNufYH9fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad_norm_threshold(optimizer: Optimizer)->float:\n",
        "    \"\"\"Get the grad norm threshold used to decide step time\"\"\"\n",
        "    items = []\n",
        "    for value, _, _ in tsa.get_param_groups_regrouped_by_key(optimizer, 'gradient_norm_threshold'):\n",
        "        items.append(value)\n",
        "    return max(items)"
      ],
      "metadata": {
        "id": "Xd-Ixexxs9_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad_norm(model: PreTrainedModel)->float:\n",
        "    \"\"\"\n",
        "    Gets the relevant norm out of the model using\n",
        "    torch utilities\n",
        "    \"\"\"\n",
        "    grads = [param.grad for param in model.parameters() if param.grad is not None]\n",
        "    return torch.nn.utils.get_total_norm(grads)"
      ],
      "metadata": {
        "id": "pZIofPLqtciv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop\n",
        "Standard PyTorch training loop as used in NLP, with schedules per batch. We abstract away the changes to logging, however."
      ],
      "metadata": {
        "id": "4GnuGjxiLZcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_progress(schedule: SynchronousSchedule,\n",
        "                    batch_idx: int,\n",
        "                    loss: float,\n",
        "                    norm: float,\n",
        "                    accum_steps: int\n",
        "                    ):\n",
        "    last_lr = schedule.get_last_lr()[0]\n",
        "    last_threshold = schedule.get_last_schedule(\"gradient_norm_threshold\")[0]\n",
        "    last_batch_size = BATCH_SIZE * accum_steps\n",
        "    msg = (f\"Batch {batch_idx+1:4d}/{TOTAL_BATCHES}\"\n",
        "          f\" | Loss: {loss.item():.4f}\"\n",
        "          f\" | LR: {last_lr:.4e}\"\n",
        "          f\" | Target_Threshold: {last_threshold:.4f}\"\n",
        "          f\" | Last Norm: {norm:.4f}\"\n",
        "          f\" | Last Accum Steps: {accum_steps}\"\n",
        "          f\" | Last Batch Size: {last_batch_size}\"\n",
        "          )\n",
        "    print(msg)\n"
      ],
      "metadata": {
        "id": "hy0WVNLw3XAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: PreTrainedModel,\n",
        "          dataloader: DataLoader,\n",
        "          optimizer: Optimizer,\n",
        "          schedule: LRScheduler,\n",
        "          ):\n",
        "    \"\"\"Train for TOTAL_BATCHES batches.\"\"\"\n",
        "    model.train()\n",
        "    data_iter = iter(dataloader)\n",
        "    accum_steps = 0\n",
        "    last_norm = 0\n",
        "    last_num_accum_steps = 0\n",
        "\n",
        "    for batch_idx in range(TOTAL_BATCHES):\n",
        "        # Get next batch\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dataloader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        # Forward pass and backwards pass\n",
        "        # Increase batch size\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        accum_steps += 1\n",
        "\n",
        "        # Optimizer steps when I hit or exceed my target\n",
        "        # Fortunately, even for vector averages, f(a*x) = a*f(x), so we can rescale\n",
        "        # externally.\n",
        "        current_mean_norm = get_grad_norm(model)/accum_steps\n",
        "        if get_grad_norm_threshold(optimizer) >= current_mean_norm:\n",
        "            # Rescale gradients as mean by dividing\n",
        "            # by number of accum steps.\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad /= accum_steps\n",
        "\n",
        "            # Run update\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            last_num_accum_steps = accum_steps\n",
        "            last_norm = current_mean_norm\n",
        "            accum_steps = 0\n",
        "\n",
        "        # Step schedules\n",
        "        schedule.step()\n",
        "\n",
        "        # Log progress\n",
        "        if (batch_idx + 1) % LOGGING_RATE == 0:\n",
        "            assert len(schedule.get_last_lr()) == 1, \"update logging system when adding param groups\"\n",
        "            report_progress(schedule, batch_idx, loss, last_norm, last_num_accum_steps)\n"
      ],
      "metadata": {
        "id": "WuKa4oRqLvbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting It All Together\n",
        "\n",
        "Create the components and train.\n"
      ],
      "metadata": {
        "id": "qifljNXpLwa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Setting up model and data...\")\n",
        "    model = make_model()\n",
        "    dataloader = make_dataloader()\n",
        "\n",
        "    print(\"Creating optimizer and schedules...\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    schedule = make_schedule(optimizer)\n",
        "\n",
        "    #print(f\"Scheduling: {schedule.schedule_names}\")\n",
        "    print(f\"Training for {TOTAL_BATCHES} batches with {WARMUP_BATCHES} warmup\")\n",
        "    print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "    train(model, dataloader, optimizer, schedule)\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "K7E4djwcLt6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}