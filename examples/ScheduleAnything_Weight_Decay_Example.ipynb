{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scheduling Weight Decay and Learning Rate in PyTorch\n",
        "\n",
        "This example demonstrates scheduling multiple hyperparameters concurrently using ScheduleAnything.\n",
        "\n",
        "We'll schedule both learning rate (cosine annealing) and weight decay (quadratic growth) to show how different hyperparameters can follow different curves during training.\n",
        "\n",
        "The most important thing to pay attention to is how simple the train loop remains. As far as that level of abstraction goes, it looks like we are just stepping a schedule like normal\n",
        "\n",
        "Do not expect this particular exercise to follow the standard behaviors of staching metrics and having eval distributions. To keep the example simple and easy to learn from, these were deliberately ignored."
      ],
      "metadata": {
        "id": "btmKKFh5BPXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup and Imports\n",
        "\n",
        "We use magic commands to ensure the environment is setup. Then we run all the needed imports. Note the usage of the cannonical ScheduleAnything import pattern:  torch-schedule-anything -> tsa\n",
        "\n",
        "```\n",
        "import torch_schedule_anything as tsa\n",
        "```"
      ],
      "metadata": {
        "id": "jTSm9yFyBgl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install -q transformers datasets torch-schedule-anything torch\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch_schedule_anything as tsa\n",
        "\n",
        "# Type hints\n",
        "from torch_schedule_anything import SynchronousSchedule\n",
        "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ],
      "metadata": {
        "id": "MxJY5n5-B_Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "For easy experimentation, we place the majority of the hyperparameters right here, though we do hardwire the dataset. For the most part, we stick to some fairly boring configurations that should be familiar boilerplate to anyone in NLP.\n",
        "\n",
        "Training duration and details are specified in terms of number of batches, learning rate has been set to something that is known to train, and the schedules are functional.\n",
        "\n",
        "### Schedule Overview\n",
        "\n",
        "Scheduling using builtins in this library generally works by specifying a number of warmup steps (in this case batches) a number of training steps, and some parameters relating to warmup targets and values.\n",
        "\n",
        "It should always be kept in mind that torch schedules are applied in terms of\n",
        "value(t) = base_hyperparameter*lambda(t), meaning you will get the base value times a multiplier as your final rate.\n",
        "\n",
        "The warmup target tells you what lambda will be when warmup finishes, while the final target tells you what it will be at end of training. Largely, the various builtin curves say how we get there. In this case, we use a cosine annealing, and a quadratic curve for learning rate and weight decay respectively.\n",
        "\n",
        "### Schedule Config\n",
        "\n",
        "We are going to adopt the view from \"Understanding Decoupled and Early Weight Decay\" that weight decay needs to get weaker as training proceeds, and additionally are going to say it needs to happen faster than learning rate. This is more or less arbitrary, however, and only used to get some sort of default\n",
        "\n",
        "### Tuning and Purpose\n",
        "\n",
        "This exists primarily to demonstrate the technology, not demonstrate a well-tuned example. This example has not been properly tuned besides verifying convergence, and as such do not treat this as having been deployed to be optimal."
      ],
      "metadata": {
        "id": "5id3Fv4nB9-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Device\n",
        "LOGGING_RATE = 5 # How frequently to print to console\n",
        "\n",
        "# Model/Pipeline config\n",
        "MODEL_NAME = \"distilbert-base-uncased\" # Model\n",
        "MAX_LENGTH = 256 # Maximum number of tokens in sample\n",
        "BATCH_SIZE = 32 # Batches in samples\n",
        "TOTAL_BATCHES = 2000 # All batches used over training\n",
        "WARMUP_BATCHES = 100 # Number of batches used for warmup\n",
        "\n",
        "# The learning rate and schedule.\n",
        "#\n",
        "# Keep in mind when reaching warmup target,\n",
        "# the actual lr value is\n",
        "# lr = BASE_LR*LR_WARMUP_TARGET =\n",
        "BASE_LR = 0.00001\n",
        "LR_WARMUP_TARGET = 1.0\n",
        "LR_FINAL_TARGET = 0.1\n",
        "\n",
        "# The weight decay schedule\n",
        "#\n",
        "# We already get some decay from decreasing\n",
        "# the learning rate. We just add a bit more\n",
        "# on top using an inverse warmup linear schedule\n",
        "BASE_WD = 0.1\n",
        "INVERSE_WARMUP_STARTING_MULTIPLIER = 2\n",
        "WD_WARMUP_TARGET = 1.0\n",
        "WD_FINAL_TARGET = 0.1"
      ],
      "metadata": {
        "id": "WdDeMlizGY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Boilerplate\n",
        "\n",
        "Largely standard boilerplate here.\n",
        "We make a model, we make an AdamW optimizer,\n",
        "we make a pipeline that loads imdb and tokenizes it"
      ],
      "metadata": {
        "id": "XN4oRZ5xHP-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model()->PreTrainedModel:\n",
        "    \"\"\"Load pretrained model with classification head.\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=2\n",
        "    )\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def make_dataloader()->DataLoader:\n",
        "    \"\"\"Load and tokenize IMDB dataset, return DataLoader.\"\"\"\n",
        "    dataset = load_dataset(\"imdb\", split=\"train\")  # Subset for faster demo\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    def tokenize(examples):\n",
        "        result = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        result[\"labels\"] = examples[\"label\"]\n",
        "        return result\n",
        "\n",
        "    dataset = dataset.map(tokenize, batched=True)\n",
        "    dataset = dataset.shuffle(seed=42)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "def make_optimizer(model: PreTrainedModel)->Optimizer:\n",
        "    \"\"\"Create optimizer with base hyperparameter values that schedules multiply against.\"\"\"\n",
        "    return AdamW(\n",
        "        model.parameters(),\n",
        "        lr=BASE_LR,\n",
        "        weight_decay=BASE_WD\n",
        "    )"
      ],
      "metadata": {
        "id": "PIZr4sAAHaxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schedule Factory, and the Novelty\n",
        "\n",
        "This is where ScheduleAnything comes in. We're going to create two different schedules and bind them to two different hyperparameters.\n",
        "\n",
        "PyTorch schedulers only work on learning rate by default. ScheduleAnything lets you bind schedulers to *any* hyperparameter using the `schedule_target` parameter.\n",
        "\n",
        "**The pattern:**\n",
        "1. Create a schedule for each hyperparameter you want to control\n",
        "2. Use `schedule_target` to specify which hyperparameter each schedule controls\n",
        "3. Wrap them in `SynchronousSchedule` to keep them coordinated\n",
        "\n",
        "These are standard PyTorch `LRScheduler` objects under the hood - we're just routing them to different hyperparameters instead of all controlling 'lr'. The result can be a schedule object that, as far as stuff downstream is concerned, still has a usable step method by duck typing, all the other method, but has some additional logging extensions if desired as well.\n",
        "\n",
        "**Crucially**, this means downstream does not need to know or care that anything weird is happening to the schedule at all. In theory, if we do not want custom logging, it does not matter in the slightest to the main training loop"
      ],
      "metadata": {
        "id": "IDBw6SCFHnLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_schedule(optimizer: Optimizer)->SynchronousSchedule:\n",
        "    \"\"\"\n",
        "    Create coordinated schedules for learning rate and weight decay.\n",
        "\n",
        "    Returns a SynchronousSchedule that steps both schedules together.\n",
        "    \"\"\"\n",
        "    # Learning rate: cosine annealing\n",
        "    lr_scheduler = tsa.cosine_annealing_with_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=LR_WARMUP_TARGET,\n",
        "        anneal_to_value=LR_FINAL_TARGET,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        schedule_target='lr'\n",
        "    )\n",
        "\n",
        "    # Weight decay: linear with inverse warmup for heavy constraint while\n",
        "    # starting up.\n",
        "    wd_scheduler = tsa.linear_schedule_with_inverse_warmup(\n",
        "        optimizer,\n",
        "        warmup_to_value=WD_WARMUP_TARGET,\n",
        "        anneal_to_value=WD_FINAL_TARGET,\n",
        "        num_warmup_steps=WARMUP_BATCHES,\n",
        "        num_training_steps=TOTAL_BATCHES,\n",
        "        warmup_multiplier=INVERSE_WARMUP_STARTING_MULTIPLIER,\n",
        "        schedule_target='weight_decay'\n",
        "    )\n",
        "\n",
        "    # Coordinate them to step together\n",
        "    return tsa.SynchronousSchedule([lr_scheduler, wd_scheduler])\n"
      ],
      "metadata": {
        "id": "aBZiNufYH9fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop\n",
        "Standard PyTorch training loop as used in NLP, with schedules per batch. We abstract away the changes to logging, however."
      ],
      "metadata": {
        "id": "4GnuGjxiLZcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_progress(schedule: SynchronousSchedule\n",
        "                    batch_idx: int,\n",
        "                    loss: float):\n",
        "    last_lr = schedule.get_last_lr()[0]\n",
        "    last_wd = schedule.get_last_schedule('weight_decay')[0]\n",
        "    msg = (f\"Batch {batch_idx+1:4d}/{TOTAL_BATCHES}\"\n",
        "          f\" | Loss: {loss.item():.4f}\"\n",
        "          f\" | LR: {last_lr:.4e}\"\n",
        "          f\" | WD: {last_wd:.4e}\"\n",
        "          )\n",
        "    print(msg)\n"
      ],
      "metadata": {
        "id": "hy0WVNLw3XAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: PreTrainedModel,\n",
        "          dataloader: DataLoader,\n",
        "          optimizer: Optimizer,\n",
        "          schedule: LRScheduler,\n",
        "          ):\n",
        "    \"\"\"Train for TOTAL_BATCHES batches.\"\"\"\n",
        "    model.train()\n",
        "    data_iter = iter(dataloader)\n",
        "\n",
        "    for batch_idx in range(TOTAL_BATCHES):\n",
        "        # Get next batch\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dataloader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Step schedules\n",
        "        schedule.step()\n",
        "\n",
        "        # Log progress\n",
        "        if (batch_idx + 1) % LOGGING_RATE == 0:\n",
        "            assert len(schedule.get_last_lr()) == 1, \"update logging system when adding param groups\"\n",
        "            report_progress(schedule, batch_idx, loss)\n"
      ],
      "metadata": {
        "id": "WuKa4oRqLvbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting It All Together\n",
        "\n",
        "Create the components and train.\n"
      ],
      "metadata": {
        "id": "qifljNXpLwa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Setting up model and data...\")\n",
        "    model = make_model()\n",
        "    dataloader = make_dataloader()\n",
        "\n",
        "    print(\"Creating optimizer and schedules...\")\n",
        "    optimizer = make_optimizer(model)\n",
        "    schedule = make_schedule(optimizer)\n",
        "\n",
        "    #print(f\"Scheduling: {schedule.schedule_names}\")\n",
        "    print(f\"Training for {TOTAL_BATCHES} batches with {WARMUP_BATCHES} warmup\")\n",
        "    print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "    train(model, dataloader, optimizer, schedule)\n",
        "\n",
        "    print(f\"\\nTraining complete!\")\n",
        "    lr = schedule.get_last_lr()[0]\n",
        "    wd = schedule.get_last_schedule('weight_decay')[0]\n",
        "    print(f\"Final LR: {lr}\")\n",
        "    print(f\"Final WD: {wd}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "K7E4djwcLt6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}