{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scheduling Weight Decay and Learning Rate in PyTorch\n",
    "\n",
    "This example demonstrates scheduling multiple hyperparameters concurrently using ScheduleAnything.\n",
    "\n",
    "We'll schedule both learning rate (cosine annealing) and weight decay (quadratic growth) to show how different hyperparameters can follow different curves during training.\n",
    "\n",
    "The most important thing to pay attention to is how simple the train loop remains. As far as that level of abstraction goes, it looks like we are just stepping a schedule like normal\n",
    "\n",
    "Do not expect this particular exercise to follow the standard behaviors of staching metrics and having eval distributions. To keep the example simple and easy to learn from, these were deliberately ignored."
   ],
   "metadata": {
    "id": "btmKKFh5BPXB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Setup and Imports\n",
    "\n",
    "We use magic commands to ensure the environment is setup. Then we run all the needed imports. Note the usage of the cannonical ScheduleAnything import pattern:  torch-schedule-anything -> tsa\n",
    "\n",
    "```\n",
    "import torch_schedule_anything as tsa\n",
    "```"
   ],
   "metadata": {
    "id": "jTSm9yFyBgl8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup\n",
    "%pip install -q transformers datasets torch-schedule-anything torch\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_schedule_anything as tsa\n",
    "\n",
    "# Type hints\n",
    "from torch_schedule_anything import SynchronousSchedule\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LRScheduler"
   ],
   "metadata": {
    "id": "MxJY5n5-B_Nw"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "For easy experimentation, we place the majority of the hyperparameters right here, though we do hardwire the dataset. For the most part, we stick to some fairly boring configurations that should be familiar boilerplate to anyone in NLP.\n",
    "\n",
    "Training duration and details are specified in terms of number of batches, learning rate has been set to something that is known to train, and the schedules are functional.\n",
    "\n",
    "### Schedule Overview\n",
    "\n",
    "Scheduling using builtins in this library generally works by specifying a number of warmup steps (in this case batches) a number of training steps, and some parameters relating to warmup targets and values.\n",
    "\n",
    "It should always be kept in mind that torch schedules are applied in terms of\n",
    "value(t) = base_hyperparameter*lambda(t), meaning you will get the base value times a multiplier as your final rate.\n",
    "\n",
    "The warmup target tells you what lambda will be when warmup finishes, while the final target tells you what it will be at end of training. Largely, the various builtin curves say how we get there. In this case, we use a cosine annealing, and a quadratic curve for learning rate and weight decay respectively.\n",
    "\n",
    "### Schedule Config\n",
    "\n",
    "We are going to adopt the view from \"Understanding Decoupled and Early Weight Decay\" that weight decay needs to get weaker as training proceeds, and additionally are going to say it needs to happen faster than learning rate. This is more or less arbitrary, however, and only used to get some sort of default\n",
    "\n",
    "### Tuning and Purpose\n",
    "\n",
    "This exists primarily to demonstrate the technology, not demonstrate a well-tuned example. This example has not been properly tuned besides verifying convergence, and as such do not treat this as having been deployed to be optimal."
   ],
   "metadata": {
    "id": "5id3Fv4nB9-K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Device\n",
    "LOGGING_RATE = 5 # How frequently to print to console\n",
    "\n",
    "# Model/Pipeline config\n",
    "MODEL_NAME = \"distilbert-base-uncased\" # Model\n",
    "MAX_LENGTH = 256 # Maximum number of tokens in sample\n",
    "BATCH_SIZE = 32 # Batches in samples\n",
    "TOTAL_BATCHES = 2000 # All batches used over training\n",
    "WARMUP_BATCHES = 100 # Number of batches used for warmup\n",
    "\n",
    "# The learning rate and schedule.\n",
    "#\n",
    "# Keep in mind when reaching warmup target,\n",
    "# the actual lr value is\n",
    "# lr = BASE_LR*LR_WARMUP_TARGET =\n",
    "BASE_LR = 0.00001\n",
    "LR_WARMUP_TARGET = 1.0\n",
    "LR_FINAL_TARGET = 0.1\n",
    "\n",
    "# The weight decay schedule\n",
    "#\n",
    "# We already get some decay from decreasing\n",
    "# the learning rate. We just add a bit more\n",
    "# on top using an inverse warmup linear schedule\n",
    "BASE_WD = 0.1\n",
    "INVERSE_WARMUP_STARTING_MULTIPLIER = 2\n",
    "WD_WARMUP_TARGET = 1.0\n",
    "WD_FINAL_TARGET = 0.1"
   ],
   "metadata": {
    "id": "WdDeMlizGY_w"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standard Boilerplate\n",
    "\n",
    "Largely standard boilerplate here.\n",
    "We make a model, we make an AdamW optimizer,\n",
    "we make a pipeline that loads imdb and tokenizes it"
   ],
   "metadata": {
    "id": "XN4oRZ5xHP-J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def make_model()->PreTrainedModel:\n",
    "    \"\"\"Load pretrained model with classification head.\"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "def make_dataloader()->DataLoader:\n",
    "    \"\"\"Load and tokenize IMDB dataset, return DataLoader.\"\"\"\n",
    "    dataset = load_dataset(\"imdb\", split=\"train\")  # Subset for faster demo\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def tokenize(examples):\n",
    "        result = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "        return result\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "def make_optimizer(model: PreTrainedModel)->Optimizer:\n",
    "    \"\"\"Create optimizer with base hyperparameter values that schedules multiply against.\"\"\"\n",
    "    return AdamW(\n",
    "        model.parameters(),\n",
    "        lr=BASE_LR,\n",
    "        weight_decay=BASE_WD\n",
    "    )"
   ],
   "metadata": {
    "id": "PIZr4sAAHaxj"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schedule Factory, and the Novelty\n",
    "\n",
    "This is where ScheduleAnything comes in. We're going to create two different schedules and bind them to two different hyperparameters.\n",
    "\n",
    "PyTorch schedulers only work on learning rate by default. ScheduleAnything lets you bind schedulers to *any* hyperparameter using the `schedule_target` parameter.\n",
    "\n",
    "**The pattern:**\n",
    "1. Create a schedule for each hyperparameter you want to control\n",
    "2. Use `schedule_target` to specify which hyperparameter each schedule controls\n",
    "3. Wrap them in `SynchronousSchedule` to keep them coordinated\n",
    "\n",
    "These are standard PyTorch `LRScheduler` objects under the hood - we're just routing them to different hyperparameters instead of all controlling 'lr'. The result can be a schedule object that, as far as stuff downstream is concerned, still has a usable step method by duck typing, all the other method, but has some additional logging extensions if desired as well.\n",
    "\n",
    "**Crucially**, this means downstream does not need to know or care that anything weird is happening to the schedule at all. In theory, if we do not want custom logging, it does not matter in the slightest to the main training loop"
   ],
   "metadata": {
    "id": "IDBw6SCFHnLm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def make_schedule(optimizer: Optimizer)->SynchronousSchedule:\n",
    "    \"\"\"\n",
    "    Create coordinated schedules for learning rate and weight decay.\n",
    "\n",
    "    Returns a SynchronousSchedule that steps both schedules together.\n",
    "    \"\"\"\n",
    "    # Learning rate: cosine annealing\n",
    "    lr_scheduler = tsa.cosine_annealing_with_warmup(\n",
    "        optimizer,\n",
    "        warmup_to_value=LR_WARMUP_TARGET,\n",
    "        anneal_to_value=LR_FINAL_TARGET,\n",
    "        num_warmup_steps=WARMUP_BATCHES,\n",
    "        num_training_steps=TOTAL_BATCHES,\n",
    "        schedule_target='lr'\n",
    "    )\n",
    "\n",
    "    # Weight decay: linear with inverse warmup for heavy constraint while\n",
    "    # starting up.\n",
    "    wd_scheduler = tsa.linear_schedule_with_inverse_warmup(\n",
    "        optimizer,\n",
    "        warmup_to_value=WD_WARMUP_TARGET,\n",
    "        anneal_to_value=WD_FINAL_TARGET,\n",
    "        num_warmup_steps=WARMUP_BATCHES,\n",
    "        num_training_steps=TOTAL_BATCHES,\n",
    "        warmup_multiplier=INVERSE_WARMUP_STARTING_MULTIPLIER,\n",
    "        schedule_target='weight_decay'\n",
    "    )\n",
    "\n",
    "    # Coordinate them to step together\n",
    "    return tsa.SynchronousSchedule([lr_scheduler, wd_scheduler])\n"
   ],
   "metadata": {
    "id": "aBZiNufYH9fy"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_schedule_statistics(schedule: LRScheduler)->str:\n",
    "  \"\"\"Reports the last statistics of the scheduling system\"\"\"\n",
    "\n",
    "  # Setup output report\n",
    "  output = \"\"\n",
    "\n",
    "  # Handle lr\n",
    "  last_lr = schedule.get_last_lr()[0]\n",
    "  output += f\" | LR: {last_lr:.4e}\"\n",
    "\n",
    "  # Handle wd\n",
    "  if isinstance(schedule, tsa.SynchronousSchedule):\n",
    "    last_wd = schedule.get_last_schedule('weight_decay')[0]\n",
    "    output += f\" | WD: {last_wd:.4e}\"\n",
    "\n",
    "  return output"
   ],
   "metadata": {
    "id": "bVlPR34LKJ0p"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Loop\n",
    "Standard PyTorch training loop as used in NLP, with schedules per batch. We abstract away the changes to logging, however."
   ],
   "metadata": {
    "id": "4GnuGjxiLZcd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model: PreTrainedModel,\n",
    "          dataloader: DataLoader,\n",
    "          optimizer: Optimizer,\n",
    "          schedule: LRScheduler,\n",
    "          ):\n",
    "    \"\"\"Train for TOTAL_BATCHES batches.\"\"\"\n",
    "    model.train()\n",
    "    data_iter = iter(dataloader)\n",
    "\n",
    "    for batch_idx in range(TOTAL_BATCHES):\n",
    "        # Get next batch\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            batch = next(data_iter)\n",
    "\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step schedules\n",
    "        schedule.step()\n",
    "\n",
    "        # Log progress\n",
    "        if (batch_idx + 1) % LOGGING_RATE == 0:\n",
    "            msg = (f\"Batch {batch_idx+1:4d}/{TOTAL_BATCHES}\"\n",
    "                  f\" | Loss: {loss.item():.4f}\"\n",
    "                  )\n",
    "            msg += get_schedule_statistics(schedule)\n",
    "            print(msg)\n"
   ],
   "metadata": {
    "id": "WuKa4oRqLvbB"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Create the components and train.\n"
   ],
   "metadata": {
    "id": "qifljNXpLwa3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    print(\"Setting up model and data...\")\n",
    "    model = make_model()\n",
    "    dataloader = make_dataloader()\n",
    "\n",
    "    print(\"Creating optimizer and schedules...\")\n",
    "    optimizer = make_optimizer(model)\n",
    "    schedule = make_schedule(optimizer)\n",
    "\n",
    "    #print(f\"Scheduling: {schedule.schedule_names}\")\n",
    "    print(f\"Training for {TOTAL_BATCHES} batches with {WARMUP_BATCHES} warmup\")\n",
    "    print(f\"Device: {DEVICE}\\n\")\n",
    "\n",
    "    train(model, dataloader, optimizer, schedule)\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    lr = schedule.get_last_lr()[0]\n",
    "    wd = schedule.get_last_schedule('weight_decay')[0]\n",
    "    print(f\"Final LR: {lr}\")\n",
    "    print(f\"Final WD: {wd}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7E4djwcLt6_",
    "outputId": "16bb72ca-cb8f-4a88-864b-0104df6de7a3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Setting up model and data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating optimizer and schedules...\n",
      "Training for 2000 batches with 100 warmup\n",
      "Device: cuda\n",
      "\n",
      "Batch    5/2000 | Loss: 0.6726 | LR: 5.0000e-07 | WD: 1.9500e-01\n",
      "Batch   10/2000 | Loss: 0.7182 | LR: 1.0000e-06 | WD: 1.9000e-01\n",
      "Batch   15/2000 | Loss: 0.6881 | LR: 1.5000e-06 | WD: 1.8500e-01\n",
      "Batch   20/2000 | Loss: 0.7019 | LR: 2.0000e-06 | WD: 1.8000e-01\n",
      "Batch   25/2000 | Loss: 0.7012 | LR: 2.5000e-06 | WD: 1.7500e-01\n",
      "Batch   30/2000 | Loss: 0.7089 | LR: 3.0000e-06 | WD: 1.7000e-01\n",
      "Batch   35/2000 | Loss: 0.7043 | LR: 3.5000e-06 | WD: 1.6500e-01\n",
      "Batch   40/2000 | Loss: 0.7020 | LR: 4.0000e-06 | WD: 1.6000e-01\n",
      "Batch   45/2000 | Loss: 0.6898 | LR: 4.5000e-06 | WD: 1.5500e-01\n",
      "Batch   50/2000 | Loss: 0.6743 | LR: 5.0000e-06 | WD: 1.5000e-01\n",
      "Batch   55/2000 | Loss: 0.6827 | LR: 5.5000e-06 | WD: 1.4500e-01\n",
      "Batch   60/2000 | Loss: 0.6794 | LR: 6.0000e-06 | WD: 1.4000e-01\n",
      "Batch   65/2000 | Loss: 0.6656 | LR: 6.5000e-06 | WD: 1.3500e-01\n",
      "Batch   70/2000 | Loss: 0.6819 | LR: 7.0000e-06 | WD: 1.3000e-01\n",
      "Batch   75/2000 | Loss: 0.6682 | LR: 7.5000e-06 | WD: 1.2500e-01\n",
      "Batch   80/2000 | Loss: 0.6354 | LR: 8.0000e-06 | WD: 1.2000e-01\n",
      "Batch   85/2000 | Loss: 0.6428 | LR: 8.5000e-06 | WD: 1.1500e-01\n",
      "Batch   90/2000 | Loss: 0.5481 | LR: 9.0000e-06 | WD: 1.1000e-01\n",
      "Batch   95/2000 | Loss: 0.5202 | LR: 9.5000e-06 | WD: 1.0500e-01\n",
      "Batch  100/2000 | Loss: 0.5651 | LR: 1.0000e-05 | WD: 1.0000e-01\n",
      "Batch  105/2000 | Loss: 0.4467 | LR: 9.9999e-06 | WD: 9.9763e-02\n",
      "Batch  110/2000 | Loss: 0.2726 | LR: 9.9997e-06 | WD: 9.9526e-02\n",
      "Batch  115/2000 | Loss: 0.4742 | LR: 9.9993e-06 | WD: 9.9289e-02\n",
      "Batch  120/2000 | Loss: 0.4278 | LR: 9.9988e-06 | WD: 9.9053e-02\n",
      "Batch  125/2000 | Loss: 0.3086 | LR: 9.9981e-06 | WD: 9.8816e-02\n",
      "Batch  130/2000 | Loss: 0.3423 | LR: 9.9972e-06 | WD: 9.8579e-02\n",
      "Batch  135/2000 | Loss: 0.2560 | LR: 9.9962e-06 | WD: 9.8342e-02\n",
      "Batch  140/2000 | Loss: 0.2455 | LR: 9.9951e-06 | WD: 9.8105e-02\n",
      "Batch  145/2000 | Loss: 0.2814 | LR: 9.9938e-06 | WD: 9.7868e-02\n",
      "Batch  150/2000 | Loss: 0.3097 | LR: 9.9923e-06 | WD: 9.7632e-02\n",
      "Batch  155/2000 | Loss: 0.6007 | LR: 9.9907e-06 | WD: 9.7395e-02\n",
      "Batch  160/2000 | Loss: 0.3489 | LR: 9.9889e-06 | WD: 9.7158e-02\n",
      "Batch  165/2000 | Loss: 0.3183 | LR: 9.9870e-06 | WD: 9.6921e-02\n",
      "Batch  170/2000 | Loss: 0.4549 | LR: 9.9849e-06 | WD: 9.6684e-02\n",
      "Batch  175/2000 | Loss: 0.2312 | LR: 9.9827e-06 | WD: 9.6447e-02\n",
      "Batch  180/2000 | Loss: 0.4062 | LR: 9.9803e-06 | WD: 9.6211e-02\n",
      "Batch  185/2000 | Loss: 0.4481 | LR: 9.9778e-06 | WD: 9.5974e-02\n",
      "Batch  190/2000 | Loss: 0.3665 | LR: 9.9751e-06 | WD: 9.5737e-02\n",
      "Batch  195/2000 | Loss: 0.2224 | LR: 9.9723e-06 | WD: 9.5500e-02\n",
      "Batch  200/2000 | Loss: 0.1915 | LR: 9.9693e-06 | WD: 9.5263e-02\n",
      "Batch  205/2000 | Loss: 0.2394 | LR: 9.9661e-06 | WD: 9.5026e-02\n",
      "Batch  210/2000 | Loss: 0.4121 | LR: 9.9628e-06 | WD: 9.4789e-02\n",
      "Batch  215/2000 | Loss: 0.2479 | LR: 9.9594e-06 | WD: 9.4553e-02\n",
      "Batch  220/2000 | Loss: 0.3070 | LR: 9.9557e-06 | WD: 9.4316e-02\n",
      "Batch  225/2000 | Loss: 0.2605 | LR: 9.9520e-06 | WD: 9.4079e-02\n",
      "Batch  230/2000 | Loss: 0.1954 | LR: 9.9481e-06 | WD: 9.3842e-02\n",
      "Batch  235/2000 | Loss: 0.3195 | LR: 9.9440e-06 | WD: 9.3605e-02\n",
      "Batch  240/2000 | Loss: 0.4377 | LR: 9.9398e-06 | WD: 9.3368e-02\n",
      "Batch  245/2000 | Loss: 0.3186 | LR: 9.9354e-06 | WD: 9.3132e-02\n",
      "Batch  250/2000 | Loss: 0.3650 | LR: 9.9309e-06 | WD: 9.2895e-02\n",
      "Batch  255/2000 | Loss: 0.2823 | LR: 9.9262e-06 | WD: 9.2658e-02\n",
      "Batch  260/2000 | Loss: 0.1729 | LR: 9.9214e-06 | WD: 9.2421e-02\n",
      "Batch  265/2000 | Loss: 0.3250 | LR: 9.9164e-06 | WD: 9.2184e-02\n",
      "Batch  270/2000 | Loss: 0.2702 | LR: 9.9113e-06 | WD: 9.1947e-02\n",
      "Batch  275/2000 | Loss: 0.3306 | LR: 9.9060e-06 | WD: 9.1711e-02\n",
      "Batch  280/2000 | Loss: 0.3124 | LR: 9.9005e-06 | WD: 9.1474e-02\n",
      "Batch  285/2000 | Loss: 0.2310 | LR: 9.8949e-06 | WD: 9.1237e-02\n",
      "Batch  290/2000 | Loss: 0.3323 | LR: 9.8892e-06 | WD: 9.1000e-02\n",
      "Batch  295/2000 | Loss: 0.2043 | LR: 9.8833e-06 | WD: 9.0763e-02\n",
      "Batch  300/2000 | Loss: 0.3847 | LR: 9.8773e-06 | WD: 9.0526e-02\n",
      "Batch  305/2000 | Loss: 0.1932 | LR: 9.8711e-06 | WD: 9.0289e-02\n",
      "Batch  310/2000 | Loss: 0.2071 | LR: 9.8647e-06 | WD: 9.0053e-02\n",
      "Batch  315/2000 | Loss: 0.2243 | LR: 9.8582e-06 | WD: 8.9816e-02\n",
      "Batch  320/2000 | Loss: 0.3501 | LR: 9.8515e-06 | WD: 8.9579e-02\n",
      "Batch  325/2000 | Loss: 0.4119 | LR: 9.8447e-06 | WD: 8.9342e-02\n",
      "Batch  330/2000 | Loss: 0.1650 | LR: 9.8378e-06 | WD: 8.9105e-02\n",
      "Batch  335/2000 | Loss: 0.2353 | LR: 9.8307e-06 | WD: 8.8868e-02\n",
      "Batch  340/2000 | Loss: 0.2169 | LR: 9.8234e-06 | WD: 8.8632e-02\n"
     ]
    }
   ]
  }
 ]
}
